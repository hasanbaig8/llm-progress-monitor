{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf9856b",
   "metadata": {},
   "source": [
    "Just run all these cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9965467",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e08b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from einops import einsum\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import time\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561baa29",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec2e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89ba94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor = torch.load('/root/llm-progress-monitor/models/probe_weights.pt')\n",
    "model_name = 'Qwen/Qwen3-4B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfff48aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(model_name, device_map=device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f67b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ema_preds(log_preds, alpha=0.5):\n",
    "    given_alpha = alpha\n",
    "    preds_list = log_preds.exp().tolist()\n",
    "    \n",
    "    ema_preds = []\n",
    "    cur_ema = None\n",
    "    for i,pred in enumerate(preds_list):\n",
    "        # Use a smooth transition from 0.5 to given_alpha, reaching given_alpha at 200 tokens\n",
    "        alpha = given_alpha\n",
    "        if cur_ema is None:\n",
    "            cur_ema = pred\n",
    "        else:\n",
    "            cur_ema = alpha*(cur_ema-1) + (1-alpha)*pred #-1 because we have stepped one token\n",
    "        ema_preds.append(cur_ema)\n",
    "    return ema_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f75a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_preds(activation, weight_tensor):\n",
    "    \"\"\"\n",
    "    Linear probe that predicts log(remaining tokens).\n",
    "    activation: [seq, d_model] tensor of activations\n",
    "    weight_tensor: [d_model] tensor of probe weights (or [d_model, 1])\n",
    "    returns: [seq] tensor of log predictions\n",
    "    \"\"\"    \n",
    "    # Handle different weight tensor shapes - flatten to 1D\n",
    "    if len(weight_tensor.shape) == 2:\n",
    "        # If weight_tensor is [d_model, 1] or [1, d_model], flatten it\n",
    "        weight_tensor = weight_tensor.flatten()\n",
    "    \n",
    "    # Simple linear projection: activation @ weights -> log(remaining tokens)\n",
    "    return einsum(activation, weight_tensor, 'seq d_model, d_model -> seq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231cba7",
   "metadata": {},
   "source": [
    "# Vibe coded UIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447d7ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bed9c1da15403d81f59d17c3445dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Text Generation Progress</h3>'), Textarea(value='', description='Prompt:', layo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create input text box for prompt\n",
    "prompt_input = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "\n",
    "# Create submit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Generate Text',\n",
    "    button_style='success',\n",
    "    tooltip='Click to start text generation',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "# Create progress bar widget\n",
    "progress_bar = widgets.FloatProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progress:',\n",
    "    bar_style='info',\n",
    "    style={'bar_color': '#20B2AA'},\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "# Create percentage label\n",
    "percentage_label = widgets.HTML(\n",
    "    value=\"<b>0.0%</b>\",\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create horizontal box for progress bar and percentage\n",
    "progress_row = widgets.HBox([progress_bar, percentage_label])\n",
    "\n",
    "# Create text widget for token display\n",
    "token_display = widgets.HTML(\n",
    "    value=\"<b>Generated tokens will appear here...</b>\",\n",
    "    placeholder='',\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create container for the widgets\n",
    "progress_container = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Text Generation Progress</h3>\"),\n",
    "    prompt_input,\n",
    "    submit_button,\n",
    "    progress_row,\n",
    "    token_display\n",
    "])\n",
    "\n",
    "# Display the widget\n",
    "display(progress_container)\n",
    "\n",
    "def on_submit_clicked(b):\n",
    "    # Reset progress\n",
    "    progress_bar.value = 0\n",
    "    percentage_label.value = \"<b>0.0%</b>\"\n",
    "    token_display.value = \"<b>Generating...</b>\"\n",
    "    \n",
    "    # Get prompt from input\n",
    "    prompt = prompt_input.value\n",
    "    # Apply chat template\n",
    "    prompt = model.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    cur_log_preds = []\n",
    "    n_tokens_generated = 0\n",
    "    generated_tokens = []\n",
    "\n",
    "    with model.generate(prompt, max_new_tokens=32768, do_sample=True) as tracer:\n",
    "        # Call .all() to apply intervention to each new token\n",
    "        with tracer.all():\n",
    "            activations = model.model.layers[15].output[0]\n",
    "            if len(activations.shape) == 1:\n",
    "                activations = activations.unsqueeze(0)\n",
    "            \n",
    "            # Get log predictions for current token position\n",
    "            log_preds = get_log_preds(activations, weight_tensor)\n",
    "            \n",
    "            # Only process the last prediction (for the newly generated token)\n",
    "            if log_preds.shape[0] > 1:\n",
    "                # Multiple positions (initial prompt), skip\n",
    "                pass\n",
    "            else:\n",
    "                # Single new token generated\n",
    "                cur_log_pred = log_preds[-1].item()\n",
    "                cur_log_preds.append(cur_log_pred)\n",
    "                \n",
    "                ema_preds = get_ema_preds(torch.tensor(cur_log_preds))\n",
    "                n_tokens_generated += 1\n",
    "                pred_tokens_remaining = ema_preds[-1]\n",
    "                pred_percent_through = n_tokens_generated / (n_tokens_generated + pred_tokens_remaining)\n",
    "\n",
    "\n",
    "                \n",
    "                token = model.lm_head.output.argmax(dim=-1).tolist()\n",
    "                token_str = model.tokenizer.decode(token[0][0])\n",
    "                generated_tokens.append(token_str)\n",
    "                \n",
    "                # Update progress bar\n",
    "                #progress_bar.value = pred_percent_through * 100\n",
    "                progress_bar.value = pred_tokens_remaining\n",
    "                \n",
    "                # Update percentage label\n",
    "                #percentage_label.value = f\"<b>{pred_percent_through*100:.1f}%</b>\"\n",
    "                percentage_label.value = f\"<b>{pred_tokens_remaining}</b>\"\n",
    "                \n",
    "                # Update token display with all generated tokens\n",
    "                tokens_html = \" \".join([f\"<span style='background-color: #e6f3ff; padding: 2px 4px; margin: 1px; border-radius: 3px;'>{token}</span>\" for token in generated_tokens])\n",
    "                token_display.value = f\"<b>Generated tokens:</b><br>{tokens_html}<br><br><b>Latest:</b> '{token_str}' | <b>Predicted:</b> {pred_percent_through*100:.1f}% through\"\n",
    "\n",
    "# Connect button click to function\n",
    "submit_button.on_click(on_submit_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4196ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07cf76e0401e44f380aec3035dcdef25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Textarea(value='', description='Prompt:', layout=Layout(height='100px', width='80%'), placehold…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple text generation with progress monitoring\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create prompt input widget\n",
    "prompt_widget = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='80%', height='100px')\n",
    ")\n",
    "\n",
    "# Create generate button\n",
    "generate_button = widgets.Button(\n",
    "    description='Generate',\n",
    "    button_style='success',\n",
    "    tooltip='Click to generate',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "# Create initial empty plot\n",
    "fig = go.FigureWidget()\n",
    "fig.add_scatter(x=[], y=[], mode='lines+markers', showlegend=False)\n",
    "# Add a separate trace for EOS marker (initially empty)\n",
    "fig.add_scatter(x=[], y=[], mode='markers', \n",
    "               marker=dict(color='green', size=15, symbol='star'),\n",
    "               showlegend=False)\n",
    "fig.update_layout(\n",
    "    title='Token Generation Progress',\n",
    "    xaxis_title='Tokens Generated',\n",
    "    yaxis_title='Predicted Remaining Tokens',\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Create token display widget\n",
    "token_display = widgets.HTML(\n",
    "    value='<div style=\"font-size: 14px; line-height: 1.8;\"><b>Generated tokens will appear here...</b></div>',\n",
    "    layout=widgets.Layout(width='80%', min_height='100px')\n",
    ")\n",
    "\n",
    "# Display input widgets, graph, and token display\n",
    "display(widgets.VBox([prompt_widget, generate_button, fig, token_display]))\n",
    "\n",
    "def get_color_for_change(change):\n",
    "    \"\"\"\n",
    "    Returns a color based on the change in predicted remaining tokens.\n",
    "    More red = increase (bad prediction), More green = decrease (good prediction)\n",
    "    \"\"\"\n",
    "    if change is None:\n",
    "        return '#e6f3ff'  # Default light blue for first token\n",
    "    \n",
    "    # Clamp change to reasonable range for color mapping\n",
    "    # Positive change = red (prediction went up), Negative = green (prediction went down)\n",
    "    clamped = max(-5, min(5, change))\n",
    "    \n",
    "    if clamped > 0:  # Increase - shades of red\n",
    "        intensity = min(1.0, clamped / 5.0)\n",
    "        # From light blue to red\n",
    "        r = int(230 + (255 - 230) * intensity)\n",
    "        g = int(243 - 243 * intensity)\n",
    "        b = int(255 - 150 * intensity)\n",
    "    else:  # Decrease - shades of green\n",
    "        intensity = min(1.0, abs(clamped) / 5.0)\n",
    "        # From light blue to green\n",
    "        r = int(230 - 130 * intensity)\n",
    "        g = int(243 - 15 * intensity)\n",
    "        b = int(255 - 155 * intensity)\n",
    "    \n",
    "    return f'rgb({r},{g},{b})'\n",
    "\n",
    "def on_generate_click(b):\n",
    "    prompt = prompt_widget.value\n",
    "    if not prompt:\n",
    "        print(\"Please enter a prompt!\")\n",
    "        return\n",
    "    \n",
    "    # Apply chat template\n",
    "    formatted_prompt = model.tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}], \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Reset graph\n",
    "    fig.data[0].x = []\n",
    "    fig.data[0].y = []\n",
    "    fig.data[1].x = []\n",
    "    fig.data[1].y = []\n",
    "    \n",
    "    # Reset token display\n",
    "    token_display.value = '<div style=\"font-size: 14px; line-height: 1.8;\"></div>'\n",
    "    \n",
    "    # Track predictions - IMPORTANT: these must be lists to persist across tracer.all() calls\n",
    "    cur_log_preds = []\n",
    "    generated_tokens = []\n",
    "    token_colors = []\n",
    "    x_vals = []\n",
    "    y_vals = []\n",
    "    prev_pred = None\n",
    "    \n",
    "    with model.generate(formatted_prompt, max_new_tokens=500, do_sample=True) as tracer:\n",
    "        # Apply intervention at each generation step\n",
    "        with tracer.all():\n",
    "            # Get activations from layer 15\n",
    "            activations = model.model.layers[15].output[0]\n",
    "            if len(activations.shape) == 1:\n",
    "                activations = activations.unsqueeze(0)\n",
    "            \n",
    "            # Get log predictions for current token position\n",
    "            log_preds = get_log_preds(activations, weight_tensor)\n",
    "            \n",
    "            # Only process single new tokens (not the initial prompt)\n",
    "            if log_preds.shape[0] == 1:\n",
    "                # Single new token generated\n",
    "                cur_log_pred = log_preds[-1].item()\n",
    "                cur_log_preds.append(cur_log_pred)\n",
    "                \n",
    "                # Apply EMA smoothing to predictions\n",
    "                ema_preds = get_ema_preds(torch.tensor(cur_log_preds))\n",
    "                n_tokens_generated = len(cur_log_preds)  # Use length of list to get current count\n",
    "                pred_tokens_remaining = ema_preds[-1]\n",
    "                \n",
    "                # Calculate change from previous prediction\n",
    "                change = None if prev_pred is None else pred_tokens_remaining - prev_pred\n",
    "                color = get_color_for_change(change)\n",
    "                prev_pred = pred_tokens_remaining\n",
    "                \n",
    "                # Get the generated token\n",
    "                token = model.lm_head.output.argmax(dim=-1).tolist()\n",
    "                token_str = model.tokenizer.decode(token[0][0])\n",
    "                generated_tokens.append(token_str)\n",
    "                token_colors.append(color)\n",
    "                \n",
    "                # Update graph data\n",
    "                x_vals.append(n_tokens_generated)\n",
    "                y_vals.append(pred_tokens_remaining)\n",
    "                fig.data[0].x = x_vals\n",
    "                fig.data[0].y = y_vals\n",
    "                \n",
    "                # Check if this is the EOS token and mark it with a star\n",
    "                if token[0][0] == model.tokenizer.eos_token_id:\n",
    "                    fig.data[1].x = [n_tokens_generated]\n",
    "                    fig.data[1].y = [pred_tokens_remaining]\n",
    "                \n",
    "                # Update token display with color-coded tokens\n",
    "                tokens_html = ''.join([\n",
    "                    f'<span style=\"display: inline-block; background-color: {token_colors[i]}; padding: 4px 8px; '\n",
    "                    f'margin: 2px; border-radius: 4px; border: 1px solid #b3d9ff; '\n",
    "                    f'font-family: monospace; white-space: pre;\">{tok}</span>'\n",
    "                    for i, tok in enumerate(generated_tokens)\n",
    "                ])\n",
    "                token_display.value = f'<div style=\"font-size: 14px; line-height: 1.8;\">{tokens_html}</div>'\n",
    "\n",
    "# Connect button to function\n",
    "generate_button.on_click(on_generate_click)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
