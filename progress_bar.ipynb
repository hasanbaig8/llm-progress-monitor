{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccf9856b",
   "metadata": {},
   "source": [
    "Just run all these cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9965467",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e08b669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from einops import einsum\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561baa29",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec2e979",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89ba94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_tensor = torch.load('/workspace/llm-progress-monitor/qwen3_4b_weight_tensor.pt')\n",
    "model_name = 'Qwen/Qwen3-4B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfff48aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2c46011ea0444f8e24e1ae5627c34e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a99039f255433da991f013f9a0ccdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a049f4f9de64378b6198597bcf22e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87acba8295f342f7ba5848a17b74a966",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4714b8b0173e40aca26b05c450b06ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LanguageModel(model_name, device_map=device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f67b7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ema_preds(log_preds, alpha=0.5):\n",
    "    given_alpha = alpha\n",
    "    preds_list = log_preds.exp().tolist()\n",
    "    \n",
    "    ema_preds = []\n",
    "    cur_ema = None\n",
    "    for i,pred in enumerate(preds_list):\n",
    "        # Use a smooth transition from 0.5 to given_alpha, reaching given_alpha at 200 tokens\n",
    "        alpha = given_alpha\n",
    "        if cur_ema is None:\n",
    "            cur_ema = pred\n",
    "        else:\n",
    "            cur_ema = alpha*(cur_ema-1) + (1-alpha)*pred #-1 because we have stepped one token\n",
    "        ema_preds.append(cur_ema)\n",
    "    return ema_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2f75a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_preds(activation, weight_tensor):\n",
    "    print(activation.shape, weight_tensor.shape, activation.dtype, weight_tensor.dtype)\n",
    "\n",
    "    return einsum(\n",
    "        einsum(activation, weight_tensor, 'seq d_model, pca d_model -> seq pca').softmax(dim=1),\n",
    "        0.5+torch.arange(weight_tensor.shape[0]).to(device, dtype=torch.bfloat16),\n",
    "        'seq pca, pca -> seq'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4231cba7",
   "metadata": {},
   "source": [
    "# Vibe coded UIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d7ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7616285e444a59bafd4fdd78fea573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Text Generation Progress</h3>'), Textarea(value='', description='Prompt:', layoâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df3cf06b3be49eda4a24453198984c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db2f4a90d44b4ba8a76e95db5bb48f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6ff090dfef4a82a50c17e115944633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/3.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fdaa3485f0243f48fd1c0f55585abed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/99.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eded344fbf064529b6a51b95ab9f3a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/3.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9aea61a651d4a7aa4a5813d42b10322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4ff926420f48328e63465cd698e0ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.Size([14, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/llm-progress-monitor/venv/lib/python3.11/site-packages/torch/functional.py:422: UserWarning: Attempting to run cuBLAS, but there was no current CUDA context! Attempting to set the primary context... (Triggered internally at /pytorch/aten/src/ATen/cuda/CublasHandlePool.cpp:179.)\n",
      "  return _VF.einsum(equation, operands)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "\n",
      "\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "Okay\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " user\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " is\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " asking\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " for\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " capital\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " of\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " France\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ".\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " I\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " know\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " that\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " France\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " is\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " a\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " country\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " in\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Europe\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " and\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " I\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " remember\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " that\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Paris\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " capital\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " is\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Paris\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ".\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " But\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " wait\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " I\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " me\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " make\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " sure\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " I\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "'m\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " not\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " mixing\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " up\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " any\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " other\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " cities\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ".\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Sometimes\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " people\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " are\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " common\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " mistakes\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "ceptions\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " like\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " maybe\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " confusing\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " it\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "'s\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Lyon\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " or\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " another\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " city\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " city\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ".\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " But\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " no\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Paris\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "'m\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " pretty\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " sure\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Paris\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " is\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " capital\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ".\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Let\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " me\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " think\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " of\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " some\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " facts\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " to\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " confirm\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ".\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " France\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " is\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " largest\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " cultural\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " and\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " cultural\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " center\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " of\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " France\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ".\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " It\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "'s\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " where\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " government\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " is\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " located\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " E\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "iff\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "el\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Tower\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " is\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " there\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " and\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " it\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "'s\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " a\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " major\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " international\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " site\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ".\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Also\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " French\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " president\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " resides\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " in\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " E\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "ly\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "se\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Palace\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " in\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Paris\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ".\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Yeah\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " that\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "'s\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " right\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ".\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " I\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " don\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "'t\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " think\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " there\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "'s\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " any\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " confusion\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " here\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ".\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " The\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " answer\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " should\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " be\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Paris\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ".\n",
      "\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "</think>\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "\n",
      "\n",
      "\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "The\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " capital\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " of\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " France\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " is\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " **\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "Paris\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "**\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ".\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " It\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " is\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " political\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " economic\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " and\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " cultural\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " heart\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " of\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " country\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " home\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " to\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " landmarks\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " landmarks\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " such\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " E\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "iff\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "el\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Tower\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Lou\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "vre\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Museum\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ",\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " and\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " E\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "ly\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "see\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " Palace\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " (\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "where\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " the\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " French\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " president\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      " resides\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      ").\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create input text box for prompt\n",
    "prompt_input = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "\n",
    "# Create submit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Generate Text',\n",
    "    button_style='success',\n",
    "    tooltip='Click to start text generation',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "# Create progress bar widget\n",
    "progress_bar = widgets.FloatProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progress:',\n",
    "    bar_style='info',\n",
    "    style={'bar_color': '#20B2AA'},\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "# Create percentage label\n",
    "percentage_label = widgets.HTML(\n",
    "    value=\"<b>0.0%</b>\",\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create horizontal box for progress bar and percentage\n",
    "progress_row = widgets.HBox([progress_bar, percentage_label])\n",
    "\n",
    "# Create text widget for token display\n",
    "token_display = widgets.HTML(\n",
    "    value=\"<b>Generated tokens will appear here...</b>\",\n",
    "    placeholder='',\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create container for the widgets\n",
    "progress_container = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Text Generation Progress</h3>\"),\n",
    "    prompt_input,\n",
    "    submit_button,\n",
    "    progress_row,\n",
    "    token_display\n",
    "])\n",
    "\n",
    "# Display the widget\n",
    "display(progress_container)\n",
    "\n",
    "def on_submit_clicked(b):\n",
    "    # Reset progress\n",
    "    progress_bar.value = 0\n",
    "    percentage_label.value = \"<b>0.0%</b>\"\n",
    "    token_display.value = \"<b>Generating...</b>\"\n",
    "    \n",
    "    # Get prompt from input\n",
    "    prompt = prompt_input.value\n",
    "    # Apply chat template\n",
    "    prompt = model.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    cur_log_preds = []\n",
    "    n_tokens_generated = 0\n",
    "    generated_tokens = []\n",
    "\n",
    "    with model.generate(prompt, max_new_tokens=32768, do_sample=True) as tracer:\n",
    "        # Call .all() to apply intervention to each new token\n",
    "        with tracer.all():\n",
    "            activations = model.model.layers[15].output[0]\n",
    "            print(activations.dtype)\n",
    "            if len(activations.shape) == 1:\n",
    "                activations = activations.unsqueeze(0)\n",
    "            preds = get_log_preds(activations, weight_tensor).tolist()\n",
    "            if len(preds) > 1:\n",
    "                pass\n",
    "            else:\n",
    "                cur_log_preds+=preds\n",
    "                ema_preds = get_ema_preds(torch.tensor(cur_log_preds))\n",
    "                n_tokens_generated+=1\n",
    "                pred_tokens_remaining = ema_preds[-1]\n",
    "                pred_percent_through = n_tokens_generated/(n_tokens_generated + pred_tokens_remaining)\n",
    "                \n",
    "                token = model.lm_head.output.argmax(dim=-1).tolist()\n",
    "                token_str = model.tokenizer.decode(token[0][0], skip_special_tokens=True)\n",
    "                print(token_str)\n",
    "                generated_tokens.append(token_str)\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.value = pred_percent_through * 100\n",
    "                \n",
    "                # Update percentage label\n",
    "                percentage_label.value = f\"<b>{pred_percent_through*100:.1f}%</b>\"\n",
    "                \n",
    "                # Update token display with all generated tokens\n",
    "                tokens_html = \" \".join([f\"<span style='background-color: #e6f3ff; padding: 2px 4px; margin: 1px; border-radius: 3px;'>{token}</span>\" for token in generated_tokens])\n",
    "                token_display.value = f\"<b>Generated tokens:</b><br>{tokens_html}<br><br><b>Latest:</b> '{token_str}' | <b>Predicted:</b> {pred_percent_through*100:.1f}% through\"\n",
    "\n",
    "# Connect button click to function\n",
    "submit_button.on_click(on_submit_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167849a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b7e6d033e5146ba8a3196f1fb60c4d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Text Generation Progress</h3>'), Textarea(value='', description='Prompt:', layoâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "torch.Size([18, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n",
      "torch.bfloat16\n",
      "torch.Size([1, 2560]) torch.Size([11, 2560]) torch.bfloat16 torch.bfloat16\n"
     ]
    },
    {
     "ename": "NNsightException",
     "evalue": "\n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_2065/3361139445.py\", line 233, in on_submit_clicked\n    raw_preds += preds\n\nUnboundLocalError: cannot access local variable 'raw_preds' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNNsightException\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 222\u001b[39m, in \u001b[36mon_submit_clicked\u001b[39m\u001b[34m(b)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;66;03m# Apply chat template\u001b[39;00m\n\u001b[32m    220\u001b[39m prompt = model.tokenizer.apply_chat_template([{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}], tokenize=\u001b[38;5;28;01mFalse\u001b[39;00m, add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtracer\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m        \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/llm-progress-monitor/venv/lib/python3.11/site-packages/nnsight/intervention/tracing/base.py:601\u001b[39m, in \u001b[36mTracer.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;66;03m# Handle the ExitTracingException (our control flow mechanism)\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exc_type \u001b[38;5;129;01mis\u001b[39;00m ExitTracingException:\n\u001b[32m    599\u001b[39m     \u001b[38;5;66;03m# This is the expected case - the traced code was intercepted\u001b[39;00m\n\u001b[32m    600\u001b[39m     \u001b[38;5;66;03m# Execute the captured code using the configured backend\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m601\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m     \u001b[38;5;66;03m# Return True to suppress the ExitTracingException\u001b[39;00m\n\u001b[32m    604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspace/llm-progress-monitor/venv/lib/python3.11/site-packages/nnsight/intervention/backends/execution.py:24\u001b[39m, in \u001b[36mExecutionBackend.__call__\u001b[39m\u001b[34m(self, tracer)\u001b[39m\n\u001b[32m     21\u001b[39m     tracer.execute(fn)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m wrap_exception(e, tracer.info) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     26\u001b[39m     Globals.exit()\n",
      "\u001b[31mNNsightException\u001b[39m: \n\nTraceback (most recent call last):\n  File \"/tmp/ipykernel_2065/3361139445.py\", line 233, in on_submit_clicked\n    raw_preds += preds\n\nUnboundLocalError: cannot access local variable 'raw_preds' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import ipywidgets as widgets\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "\n",
    "# Create input text box for prompt\n",
    "prompt_input = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "\n",
    "# Create input text box for EMA factor\n",
    "ema_input = widgets.FloatText(\n",
    "    value=0.9,\n",
    "    description='EMA Factor:',\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    tooltip='Exponential Moving Average factor (0.0 to 1.0)',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Create submit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Generate Text',\n",
    "    button_style='success',\n",
    "    tooltip='Click to start text generation',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "# Create progress bar widget\n",
    "progress_bar = widgets.FloatProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progress:',\n",
    "    bar_style='info',\n",
    "    style={'bar_color': '#20B2AA'},\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "# Create percentage label\n",
    "percentage_label = widgets.HTML(\n",
    "    value=\"<b>0.0%</b>\",\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create horizontal box for progress bar and percentage\n",
    "progress_row = widgets.HBox([progress_bar, percentage_label])\n",
    "\n",
    "# Create text widget for token display\n",
    "token_display = widgets.HTML(\n",
    "    value=\"<b>Generated tokens will appear here...</b>\",\n",
    "    placeholder='',\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create graph widget for prediction history\n",
    "graph_widget = go.FigureWidget()\n",
    "\n",
    "# Create container for the widgets\n",
    "progress_container = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Text Generation Progress</h3>\"),\n",
    "    prompt_input,\n",
    "    ema_input,\n",
    "    submit_button,\n",
    "    progress_row,\n",
    "    token_display,\n",
    "    widgets.HTML(\"<h4>Prediction History</h4>\"),\n",
    "    graph_widget\n",
    "])\n",
    "\n",
    "# Display the widget\n",
    "display(progress_container)\n",
    "\n",
    "# Global variables to store generation data\n",
    "raw_log_preds = []\n",
    "stored_generated_tokens = []\n",
    "stored_n_tokens = 0\n",
    "\n",
    "def update_graph_with_ema(ema_factor):\n",
    "    \"\"\"Update the graph and displays with a new EMA factor.\"\"\"\n",
    "    global raw_log_preds, stored_generated_tokens, stored_n_tokens\n",
    "    \n",
    "    if len(raw_log_preds) == 0:\n",
    "        return\n",
    "    \n",
    "    # Recalculate predictions with new EMA\n",
    "    ema_preds = get_ema_preds(torch.tensor(raw_log_preds), alpha=ema_factor)\n",
    "    \n",
    "    prediction_history = []\n",
    "    token_counts = []\n",
    "    \n",
    "    for i, ema_pred in enumerate(ema_preds):\n",
    "        n_tokens_generated = i + 1\n",
    "        pred_tokens_remaining = ema_pred\n",
    "        predicted_total_tokens = n_tokens_generated + pred_tokens_remaining\n",
    "        prediction_history.append(predicted_total_tokens)\n",
    "        token_counts.append(n_tokens_generated)\n",
    "    \n",
    "    # Update the graph\n",
    "    if len(prediction_history) > 1:\n",
    "        # Create hover text with context tokens\n",
    "        hover_texts = []\n",
    "        for i in range(len(stored_generated_tokens)):\n",
    "            # Get 5 tokens before and after (if available)\n",
    "            start_idx = max(0, i - 5)\n",
    "            end_idx = min(len(stored_generated_tokens), i + 6)\n",
    "            \n",
    "            context_tokens = []\n",
    "            for j in range(start_idx, end_idx):\n",
    "                token_clean = stored_generated_tokens[j].replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '\"').replace('&#x27;', \"'\")\n",
    "                if j == i:\n",
    "                    context_tokens.append(f\"<b>{token_clean}</b>\")\n",
    "                else:\n",
    "                    context_tokens.append(token_clean)\n",
    "            \n",
    "            context_str = \" \".join(context_tokens)\n",
    "            hover_text = f\"Token {i+1}: {context_str}<br>Predicted Total: {prediction_history[i]:.0f}\"\n",
    "            hover_texts.append(hover_text)\n",
    "        \n",
    "        # Update graph with new data\n",
    "        graph_widget.data = []\n",
    "        graph_widget.add_trace(go.Scatter(\n",
    "            x=token_counts,\n",
    "            y=prediction_history,\n",
    "            mode='lines+markers',\n",
    "            name='Predicted Total Tokens',\n",
    "            line=dict(color='blue', width=2),\n",
    "            marker=dict(size=6),\n",
    "            hovertemplate='%{customdata}<extra></extra>',\n",
    "            customdata=hover_texts\n",
    "        ))\n",
    "        \n",
    "        # Add actual final point\n",
    "        graph_widget.add_trace(go.Scatter(\n",
    "            x=[stored_n_tokens],\n",
    "            y=[stored_n_tokens],\n",
    "            mode='markers',\n",
    "            name='Actual Final',\n",
    "            marker=dict(size=10, color='green', symbol='star'),\n",
    "            hovertemplate=f'Actual completion: {stored_n_tokens} tokens<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        # Update layout\n",
    "        graph_widget.update_layout(\n",
    "            title=f'Token Prediction vs Reality (EMA={ema_factor:.2f})',\n",
    "            xaxis_title='Token Number',\n",
    "            yaxis_title='Predicted Total Tokens',\n",
    "            height=400,\n",
    "            showlegend=True,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Update token display with final statistics\n",
    "        final_pred = prediction_history[-1]\n",
    "        accuracy = (stored_n_tokens/final_pred)*100\n",
    "        token_display.value = f\"<b>Generation complete!</b><br><b>Total tokens:</b> {stored_n_tokens}<br><b>Final prediction:</b> {final_pred:.0f} tokens<br><b>Accuracy:</b> {accuracy:.1f}%<br><b>Current EMA:</b> {ema_factor:.2f}\"\n",
    "\n",
    "def on_ema_changed(change):\n",
    "    \"\"\"Handle EMA input changes.\"\"\"\n",
    "    update_graph_with_ema(change['new'])\n",
    "\n",
    "# Connect EMA input to update function\n",
    "ema_input.observe(on_ema_changed, names='value')\n",
    "\n",
    "def on_submit_clicked(b):\n",
    "    global raw_log_preds, stored_generated_tokens, stored_n_tokens\n",
    "    \n",
    "    # Reset progress\n",
    "    progress_bar.value = 0\n",
    "    percentage_label.value = \"<b>0.0%</b>\"\n",
    "    token_display.value = \"<b>Generating...</b>\"\n",
    "    \n",
    "    # Clear the graph\n",
    "    graph_widget.data = []\n",
    "    \n",
    "    # Reset global storage\n",
    "    raw_log_preds = []\n",
    "    stored_generated_tokens = []\n",
    "    stored_n_tokens = 0\n",
    "    \n",
    "    # Initialize lists to track predictions over time\n",
    "    prediction_history = []\n",
    "    token_counts = []\n",
    "    \n",
    "    # Get prompt from input\n",
    "    prompt = prompt_input.value\n",
    "    # Get EMA factor from input\n",
    "    ema_factor = ema_input.value\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = model.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    cur_log_preds = []\n",
    "    n_tokens_generated = 0\n",
    "    generated_tokens = []\n",
    "\n",
    "    with model.generate(prompt, max_new_tokens=32768, do_sample=True) as tracer:\n",
    "        # Call .all() to apply intervention to each new token\n",
    "        with tracer.all():\n",
    "            activations = model.model.layers[15].output[0]\n",
    "            print(activations.dtype)\n",
    "            if len(activations.shape) == 1:\n",
    "                activations = activations.unsqueeze(0)\n",
    "            \n",
    "            # Save predictions within nnsight context\n",
    "            preds_saved = get_log_preds(activations, weight_tensor).save()\n",
    "            token_saved = model.lm_head.output.argmax(dim=-1).save()\n",
    "            \n",
    "            preds = preds_saved.tolist()\n",
    "            if len(preds) > 1:\n",
    "                pass\n",
    "            else:\n",
    "                cur_log_preds+=preds\n",
    "                raw_log_preds.append(preds[0])  # Store raw predictions globally\n",
    "                \n",
    "                ema_preds = get_ema_preds(torch.tensor(cur_log_preds), alpha=ema_factor)\n",
    "                n_tokens_generated+=1\n",
    "                pred_tokens_remaining = ema_preds[-1]\n",
    "                predicted_total_tokens = n_tokens_generated + pred_tokens_remaining\n",
    "                pred_percent_through = n_tokens_generated/(n_tokens_generated + pred_tokens_remaining)\n",
    "                \n",
    "                # Store prediction data for highlighting\n",
    "                prediction_history.append(predicted_total_tokens)\n",
    "                token_counts.append(n_tokens_generated)\n",
    "                \n",
    "                token = token_saved.tolist()\n",
    "                token_str = model.tokenizer.decode(token[0][0], skip_special_tokens=False)\n",
    "                # Escape HTML entities in token string for safe display\n",
    "                token_str_escaped = token_str.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;').replace(\"'\", '&#x27;')\n",
    "                print(token_str)\n",
    "                generated_tokens.append(token_str_escaped)\n",
    "                stored_generated_tokens.append(token_str_escaped)  # Store globally\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.value = pred_percent_through * 100\n",
    "                \n",
    "                # Update percentage label\n",
    "                percentage_label.value = f\"<b>{pred_percent_through*100:.1f}%</b>\"\n",
    "                \n",
    "                # Create highlighted token display\n",
    "                highlighted_tokens = []\n",
    "                for i, token in enumerate(generated_tokens):\n",
    "                    # Calculate percentage change if we have history\n",
    "                    highlight_color = \"#e6f3ff\"  # Default light blue\n",
    "                    \n",
    "                    if i > 0 and i < len(prediction_history):\n",
    "                        change = prediction_history[i] - prediction_history[i-1]\n",
    "                        percent_change = abs(change / prediction_history[i-1]) * 100 if prediction_history[i-1] != 0 else 0\n",
    "                        \n",
    "                        # Color code based on prediction change magnitude\n",
    "                        if percent_change > 15:\n",
    "                            if change > 0:\n",
    "                                highlight_color = \"#ffcccc\"  # Light red for large increases\n",
    "                            else:\n",
    "                                highlight_color = \"#ccffcc\"  # Light green for large decreases\n",
    "                        elif percent_change > 5:\n",
    "                            if change > 0:\n",
    "                                highlight_color = \"#ffe6cc\"  # Light orange for medium increases\n",
    "                            else:\n",
    "                                highlight_color = \"#e6ffcc\"  # Light yellow-green for medium decreases\n",
    "                    \n",
    "                    highlighted_tokens.append(f\"<span style='background-color: {highlight_color}; padding: 2px 4px; margin: 1px; border-radius: 3px;'>{token}</span>\")\n",
    "                \n",
    "                tokens_html = \" \".join(highlighted_tokens)\n",
    "                token_display.value = f\"<b>Generated tokens:</b><br>{tokens_html}<br><br><b>Latest:</b> '{token_str_escaped}' | <b>Predicted Total:</b> {predicted_total_tokens:.0f} tokens | <b>Progress:</b> {pred_percent_through*100:.1f}%<br><br><small><b>Color coding:</b> <span style='background-color: #e6f3ff; padding: 2px;'>Normal</span> <span style='background-color: #ffe6cc; padding: 2px;'>Med. increase</span> <span style='background-color: #ffcccc; padding: 2px;'>Large increase</span> <span style='background-color: #e6ffcc; padding: 2px;'>Med. decrease</span> <span style='background-color: #ccffcc; padding: 2px;'>Large decrease</span></small>\"\n",
    "                \n",
    "                # Update the graph with current predictions\n",
    "                if len(prediction_history) > 1:\n",
    "                    # Create hover text with context tokens\n",
    "                    hover_texts = []\n",
    "                    for i in range(len(generated_tokens)):\n",
    "                        # Get 5 tokens before and after (if available)\n",
    "                        start_idx = max(0, i - 5)\n",
    "                        end_idx = min(len(generated_tokens), i + 6)\n",
    "                        \n",
    "                        context_tokens = []\n",
    "                        for j in range(start_idx, end_idx):\n",
    "                            token_clean = generated_tokens[j].replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '\"').replace('&#x27;', \"'\")\n",
    "                            if j == i:\n",
    "                                context_tokens.append(f\"<b>{token_clean}</b>\")\n",
    "                            else:\n",
    "                                context_tokens.append(token_clean)\n",
    "                        \n",
    "                        context_str = \" \".join(context_tokens)\n",
    "                        hover_text = f\"Token {i+1}: {context_str}<br>Predicted Total: {prediction_history[i]:.0f}\"\n",
    "                        hover_texts.append(hover_text)\n",
    "                    \n",
    "                    # Update graph with new data\n",
    "                    graph_widget.data = []\n",
    "                    graph_widget.add_trace(go.Scatter(\n",
    "                        x=token_counts,\n",
    "                        y=prediction_history,\n",
    "                        mode='lines+markers',\n",
    "                        name='Predicted Total Tokens',\n",
    "                        line=dict(color='blue', width=2),\n",
    "                        marker=dict(size=6),\n",
    "                        hovertemplate='%{customdata}<extra></extra>',\n",
    "                        customdata=hover_texts\n",
    "                    ))\n",
    "                    \n",
    "                    # Add a horizontal line showing actual tokens generated so far\n",
    "                    graph_widget.add_trace(go.Scatter(\n",
    "                        x=[token_counts[0], token_counts[-1]],\n",
    "                        y=[n_tokens_generated, n_tokens_generated],\n",
    "                        mode='lines',\n",
    "                        name='Current Progress',\n",
    "                        line=dict(color='red', width=2, dash='dash'),\n",
    "                        hovertemplate='Current tokens generated: %{y}<extra></extra>'\n",
    "                    ))\n",
    "                    \n",
    "                    # Update layout\n",
    "                    graph_widget.update_layout(\n",
    "                        title='Token Prediction Over Time',\n",
    "                        xaxis_title='Token Number',\n",
    "                        yaxis_title='Predicted Total Tokens',\n",
    "                        height=400,\n",
    "                        showlegend=True,\n",
    "                        hovermode='closest'\n",
    "                    )\n",
    "    \n",
    "    # Store final token count globally\n",
    "    stored_n_tokens = n_tokens_generated\n",
    "    \n",
    "    # After generation is complete, display prediction history\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL PREDICTION HISTORY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nGenerated {n_tokens_generated} tokens total\")\n",
    "    print(f\"Final prediction was {predicted_total_tokens:.0f} tokens\")\n",
    "    print(f\"Accuracy: {(n_tokens_generated/predicted_total_tokens)*100:.1f}%\")\n",
    "    print(f\"EMA factor used: {ema_factor}\")\n",
    "    \n",
    "    print(\"\\nFull Prediction History:\")\n",
    "    print(\"Token# | Predicted Total | Change | % Change | Token\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, (count, pred_total, token) in enumerate(zip(token_counts, prediction_history, generated_tokens)):\n",
    "        if i == 0:\n",
    "            change = 0\n",
    "            percent_change = 0\n",
    "        else:\n",
    "            change = pred_total - prediction_history[i-1]\n",
    "            percent_change = abs(change / prediction_history[i-1]) * 100 if prediction_history[i-1] != 0 else 0\n",
    "        \n",
    "        # Display token in a safe way for console output (remove escaping for print)\n",
    "        token_for_print = token.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '\"').replace('&#x27;', \"'\")\n",
    "        \n",
    "        # Highlight tokens with large percentage changes (>10%)\n",
    "        if percent_change > 10:\n",
    "            if change > 0:\n",
    "                # Large increase - bold green\n",
    "                token_display_str = f\"\\033[1m\\033[92m{token_for_print}\\033[0m\"\n",
    "                change_str = f\"\\033[1m\\033[92m+{change:.0f}\\033[0m\"\n",
    "            else:\n",
    "                # Large decrease - bold red  \n",
    "                token_display_str = f\"\\033[1m\\033[91m{token_for_print}\\033[0m\"\n",
    "                change_str = f\"\\033[1m\\033[91m{change:.0f}\\033[0m\"\n",
    "        else:\n",
    "            token_display_str = token_for_print\n",
    "            if change > 0:\n",
    "                change_str = f\"+{change:.0f}\"\n",
    "            else:\n",
    "                change_str = f\"{change:.0f}\"\n",
    "        \n",
    "        print(f\"{count:5d}  | {pred_total:13.0f}   | {change_str:8s} | {percent_change:6.1f}%  | {token_display_str}\")\n",
    "    \n",
    "    # Update graph with final results using current EMA\n",
    "    update_graph_with_ema(ema_factor)\n",
    "\n",
    "# Connect button click to function\n",
    "submit_button.on_click(on_submit_clicked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e555b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
