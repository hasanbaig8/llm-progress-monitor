{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f73bb0e",
   "metadata": {},
   "source": [
    "# Progress Bar with Linear Regression Probe\n",
    "\n",
    "This notebook uses a simple linear probe that directly predicts log(tokens_remaining) instead of classification with binning.\n",
    "\n",
    "Just run all these cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb495f6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50b3b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting plotly\n",
      "Collecting plotly\n",
      "  Downloading plotly-6.3.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "  Downloading plotly-6.3.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (9.6.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (9.6.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-2.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-2.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from plotly) (25.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from plotly) (25.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.15.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━\u001b[0m \u001b[32m112.6/139.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mDownloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading plotly-6.3.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/9.8 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mDownloading plotly-6.3.1-py3-none-any.whl (9.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/216.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0mDownloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.6.0-py3-none-any.whl (408 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-2.6.0-py3-none-any.whl (408 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.4/408.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.4/408.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m145.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0mDownloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, narwhals, jupyterlab_widgets, plotly, ipywidgets\n",
      "Installing collected packages: widgetsnbextension, narwhals, jupyterlab_widgets, plotly, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 narwhals-2.6.0 plotly-6.3.1 widgetsnbextension-4.0.14\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 narwhals-2.6.0 plotly-6.3.1 widgetsnbextension-4.0.14\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets plotly anywidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af922904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from einops import einsum\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import time\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d0d918",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23020c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6b5eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe weights shape: torch.Size([1, 2560])\n"
     ]
    }
   ],
   "source": [
    "# Load linear regression probe weights (1D vector for direct prediction)\n",
    "probe_weights = torch.load('/root/llm-progress-monitor/models/probe_weights.pt')\n",
    "print(f\"Probe weights shape: {probe_weights.shape}\")\n",
    "model_name = 'Qwen/Qwen3-4B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59988a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(model_name, device_map=device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0fc4fc",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe0acd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ema_preds(log_preds, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Apply exponential moving average to predictions.\n",
    "    log_preds: tensor of log(tokens_remaining) predictions\n",
    "    \"\"\"\n",
    "    given_alpha = alpha\n",
    "    preds_list = log_preds.exp().tolist()\n",
    "    \n",
    "    ema_preds = []\n",
    "    cur_ema = None\n",
    "    for i, pred in enumerate(preds_list):\n",
    "        alpha = given_alpha\n",
    "        if cur_ema is None:\n",
    "            cur_ema = pred\n",
    "        else:\n",
    "            cur_ema = alpha*(cur_ema-1) + (1-alpha)*pred  # -1 because we have stepped one token\n",
    "        ema_preds.append(cur_ema)\n",
    "    return ema_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b889c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_preds(activation, probe_weights):\n",
    "    \"\"\"\n",
    "    Get log(tokens_remaining) predictions using linear regression probe.\n",
    "    \n",
    "    Args:\n",
    "        activation: [seq, d_model] tensor of model activations\n",
    "        probe_weights: [d_model] tensor of linear probe weights\n",
    "    \n",
    "    Returns:\n",
    "        [seq] tensor of log(tokens_remaining) predictions\n",
    "    \"\"\"\n",
    "    print(f\"Activation shape: {activation.shape}, dtype: {activation.dtype}\")\n",
    "    print(f\"Probe weights shape: {probe_weights.shape}, dtype: {probe_weights.dtype}\")\n",
    "    \n",
    "    # Simple dot product for direct regression prediction\n",
    "    log_preds = einsum(\n",
    "        activation, \n",
    "        probe_weights, \n",
    "        'seq d_model, d_model -> seq'\n",
    "    )\n",
    "    \n",
    "    return log_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef0cb7",
   "metadata": {},
   "source": [
    "# Interactive UI with Progress Tracking and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eeeed28f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Please install anywidget to use the FigureWidget class",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     53\u001b[39m token_display = widgets.HTML(\n\u001b[32m     54\u001b[39m     value=\u001b[33m\"\u001b[39m\u001b[33m<b>Generated tokens will appear here...</b>\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     55\u001b[39m     placeholder=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     56\u001b[39m     description=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     57\u001b[39m )\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Create graph widget for prediction history\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m graph_widget = \u001b[43mgo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mFigureWidget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Create container for the widgets\u001b[39;00m\n\u001b[32m     63\u001b[39m progress_container = widgets.VBox([\n\u001b[32m     64\u001b[39m     widgets.HTML(\u001b[33m\"\u001b[39m\u001b[33m<h3>Text Generation Progress (Linear Regression Probe)</h3>\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     65\u001b[39m     prompt_input,\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m     graph_widget\n\u001b[32m     72\u001b[39m ])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/llm-progress-monitor/.venv/lib/python3.11/site-packages/plotly/missing_anywidget.py:13\u001b[39m, in \u001b[36mFigureWidget.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPlease install anywidget to use the FigureWidget class\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: Please install anywidget to use the FigureWidget class"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import ipywidgets as widgets\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create input text box for prompt\n",
    "prompt_input = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "\n",
    "# Create input text box for EMA factor\n",
    "ema_input = widgets.FloatText(\n",
    "    value=0.9,\n",
    "    description='EMA Factor:',\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    tooltip='Exponential Moving Average factor (0.0 to 1.0)',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Create submit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Generate Text',\n",
    "    button_style='success',\n",
    "    tooltip='Click to start text generation',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "# Create progress bar widget\n",
    "progress_bar = widgets.FloatProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progress:',\n",
    "    bar_style='info',\n",
    "    style={'bar_color': '#20B2AA'},\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "# Create percentage label\n",
    "percentage_label = widgets.HTML(\n",
    "    value=\"<b>0.0%</b>\",\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create horizontal box for progress bar and percentage\n",
    "progress_row = widgets.HBox([progress_bar, percentage_label])\n",
    "\n",
    "# Create text widget for token display\n",
    "token_display = widgets.HTML(\n",
    "    value=\"<b>Generated tokens will appear here...</b>\",\n",
    "    placeholder='',\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create graph widget for prediction history\n",
    "graph_widget = go.FigureWidget()\n",
    "\n",
    "# Create container for the widgets\n",
    "progress_container = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Text Generation Progress (Linear Regression Probe)</h3>\"),\n",
    "    prompt_input,\n",
    "    ema_input,\n",
    "    submit_button,\n",
    "    progress_row,\n",
    "    token_display,\n",
    "    widgets.HTML(\"<h4>Prediction History</h4>\"),\n",
    "    graph_widget\n",
    "])\n",
    "\n",
    "# Display the widget\n",
    "display(progress_container)\n",
    "\n",
    "# Global variables to store generation data\n",
    "raw_log_preds = []\n",
    "stored_generated_tokens = []\n",
    "stored_n_tokens = 0\n",
    "\n",
    "def update_graph_with_ema(ema_factor):\n",
    "    \"\"\"Update the graph and displays with a new EMA factor.\"\"\"\n",
    "    global raw_log_preds, stored_generated_tokens, stored_n_tokens\n",
    "    \n",
    "    if len(raw_log_preds) == 0:\n",
    "        return\n",
    "    \n",
    "    # Recalculate predictions with new EMA\n",
    "    ema_preds = get_ema_preds(torch.tensor(raw_log_preds), alpha=ema_factor)\n",
    "    \n",
    "    prediction_history = []\n",
    "    token_counts = []\n",
    "    \n",
    "    for i, ema_pred in enumerate(ema_preds):\n",
    "        n_tokens_generated = i + 1\n",
    "        pred_tokens_remaining = ema_pred\n",
    "        predicted_total_tokens = n_tokens_generated + pred_tokens_remaining\n",
    "        prediction_history.append(predicted_total_tokens)\n",
    "        token_counts.append(n_tokens_generated)\n",
    "    \n",
    "    # Update the graph\n",
    "    if len(prediction_history) > 1:\n",
    "        # Create hover text with context tokens\n",
    "        hover_texts = []\n",
    "        for i in range(len(stored_generated_tokens)):\n",
    "            # Get 5 tokens before and after (if available)\n",
    "            start_idx = max(0, i - 5)\n",
    "            end_idx = min(len(stored_generated_tokens), i + 6)\n",
    "            \n",
    "            context_tokens = []\n",
    "            for j in range(start_idx, end_idx):\n",
    "                token_clean = stored_generated_tokens[j].replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '\"').replace('&#x27;', \"'\")\n",
    "                if j == i:\n",
    "                    context_tokens.append(f\"<b>{token_clean}</b>\")\n",
    "                else:\n",
    "                    context_tokens.append(token_clean)\n",
    "            \n",
    "            context_str = \" \".join(context_tokens)\n",
    "            hover_text = f\"Token {i+1}: {context_str}<br>Predicted Total: {prediction_history[i]:.0f}\"\n",
    "            hover_texts.append(hover_text)\n",
    "        \n",
    "        # Update graph with new data\n",
    "        graph_widget.data = []\n",
    "        graph_widget.add_trace(go.Scatter(\n",
    "            x=token_counts,\n",
    "            y=prediction_history,\n",
    "            mode='lines+markers',\n",
    "            name='Predicted Total Tokens',\n",
    "            line=dict(color='blue', width=2),\n",
    "            marker=dict(size=6),\n",
    "            hovertemplate='%{customdata}<extra></extra>',\n",
    "            customdata=hover_texts\n",
    "        ))\n",
    "        \n",
    "        # Add actual final point\n",
    "        graph_widget.add_trace(go.Scatter(\n",
    "            x=[stored_n_tokens],\n",
    "            y=[stored_n_tokens],\n",
    "            mode='markers',\n",
    "            name='Actual Final',\n",
    "            marker=dict(size=10, color='green', symbol='star'),\n",
    "            hovertemplate=f'Actual completion: {stored_n_tokens} tokens<extra></extra>'\n",
    "        ))\n",
    "        \n",
    "        # Update layout\n",
    "        graph_widget.update_layout(\n",
    "            title=f'Token Prediction vs Reality (EMA={ema_factor:.2f})',\n",
    "            xaxis_title='Token Number',\n",
    "            yaxis_title='Predicted Total Tokens',\n",
    "            height=400,\n",
    "            showlegend=True,\n",
    "            hovermode='closest'\n",
    "        )\n",
    "        \n",
    "        # Update token display with final statistics\n",
    "        final_pred = prediction_history[-1]\n",
    "        accuracy = (stored_n_tokens/final_pred)*100\n",
    "        token_display.value = f\"<b>Generation complete!</b><br><b>Total tokens:</b> {stored_n_tokens}<br><b>Final prediction:</b> {final_pred:.0f} tokens<br><b>Accuracy:</b> {accuracy:.1f}%<br><b>Current EMA:</b> {ema_factor:.2f}\"\n",
    "\n",
    "def on_ema_changed(change):\n",
    "    \"\"\"Handle EMA input changes.\"\"\"\n",
    "    update_graph_with_ema(change['new'])\n",
    "\n",
    "# Connect EMA input to update function\n",
    "ema_input.observe(on_ema_changed, names='value')\n",
    "\n",
    "def on_submit_clicked(b):\n",
    "    global raw_log_preds, stored_generated_tokens, stored_n_tokens\n",
    "    \n",
    "    # Reset progress\n",
    "    progress_bar.value = 0\n",
    "    percentage_label.value = \"<b>0.0%</b>\"\n",
    "    token_display.value = \"<b>Generating...</b>\"\n",
    "    \n",
    "    # Clear the graph\n",
    "    graph_widget.data = []\n",
    "    \n",
    "    # Reset global storage\n",
    "    raw_log_preds = []\n",
    "    stored_generated_tokens = []\n",
    "    stored_n_tokens = 0\n",
    "    \n",
    "    # Initialize lists to track predictions over time\n",
    "    prediction_history = []\n",
    "    token_counts = []\n",
    "    \n",
    "    # Get prompt from input\n",
    "    prompt = prompt_input.value\n",
    "    # Get EMA factor from input\n",
    "    ema_factor = ema_input.value\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = model.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    cur_log_preds = []\n",
    "    n_tokens_generated = 0\n",
    "    generated_tokens = []\n",
    "\n",
    "    with model.generate(prompt, max_new_tokens=32768, do_sample=True) as tracer:\n",
    "        # Call .all() to apply intervention to each new token\n",
    "        with tracer.all():\n",
    "            activations = model.model.layers[15].output[0]\n",
    "            print(activations.dtype)\n",
    "            if len(activations.shape) == 1:\n",
    "                activations = activations.unsqueeze(0)\n",
    "            \n",
    "            # Save predictions within nnsight context\n",
    "            preds_saved = get_log_preds(activations, probe_weights).save()\n",
    "            token_saved = model.lm_head.output.argmax(dim=-1).save()\n",
    "            \n",
    "            preds = preds_saved.tolist()\n",
    "            if len(preds) > 1:\n",
    "                pass\n",
    "            else:\n",
    "                cur_log_preds += preds\n",
    "                raw_log_preds.append(preds[0])  # Store raw predictions globally\n",
    "                \n",
    "                ema_preds = get_ema_preds(torch.tensor(cur_log_preds), alpha=ema_factor)\n",
    "                n_tokens_generated += 1\n",
    "                pred_tokens_remaining = ema_preds[-1]\n",
    "                predicted_total_tokens = n_tokens_generated + pred_tokens_remaining\n",
    "                pred_percent_through = n_tokens_generated/(n_tokens_generated + pred_tokens_remaining)\n",
    "                \n",
    "                # Store prediction data for highlighting\n",
    "                prediction_history.append(predicted_total_tokens)\n",
    "                token_counts.append(n_tokens_generated)\n",
    "                \n",
    "                token = token_saved.tolist()\n",
    "                token_str = model.tokenizer.decode(token[0][0], skip_special_tokens=False)\n",
    "                # Escape HTML entities in token string for safe display\n",
    "                token_str_escaped = token_str.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;').replace(\"'\", '&#x27;')\n",
    "                print(token_str)\n",
    "                generated_tokens.append(token_str_escaped)\n",
    "                stored_generated_tokens.append(token_str_escaped)  # Store globally\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.value = pred_percent_through * 100\n",
    "                \n",
    "                # Update percentage label\n",
    "                percentage_label.value = f\"<b>{pred_percent_through*100:.1f}%</b>\"\n",
    "                \n",
    "                # Create highlighted token display\n",
    "                highlighted_tokens = []\n",
    "                for i, token in enumerate(generated_tokens):\n",
    "                    # Calculate percentage change if we have history\n",
    "                    highlight_color = \"#e6f3ff\"  # Default light blue\n",
    "                    \n",
    "                    if i > 0 and i < len(prediction_history):\n",
    "                        change = prediction_history[i] - prediction_history[i-1]\n",
    "                        percent_change = abs(change / prediction_history[i-1]) * 100 if prediction_history[i-1] != 0 else 0\n",
    "                        \n",
    "                        # Color code based on prediction change magnitude\n",
    "                        if percent_change > 15:\n",
    "                            if change > 0:\n",
    "                                highlight_color = \"#ffcccc\"  # Light red for large increases\n",
    "                            else:\n",
    "                                highlight_color = \"#ccffcc\"  # Light green for large decreases\n",
    "                        elif percent_change > 5:\n",
    "                            if change > 0:\n",
    "                                highlight_color = \"#ffe6cc\"  # Light orange for medium increases\n",
    "                            else:\n",
    "                                highlight_color = \"#e6ffcc\"  # Light yellow-green for medium decreases\n",
    "                    \n",
    "                    highlighted_tokens.append(f\"<span style='background-color: {highlight_color}; padding: 2px 4px; margin: 1px; border-radius: 3px;'>{token}</span>\")\n",
    "                \n",
    "                tokens_html = \" \".join(highlighted_tokens)\n",
    "                token_display.value = f\"<b>Generated tokens:</b><br>{tokens_html}<br><br><b>Latest:</b> '{token_str_escaped}' | <b>Predicted Total:</b> {predicted_total_tokens:.0f} tokens | <b>Progress:</b> {pred_percent_through*100:.1f}%<br><br><small><b>Color coding:</b> <span style='background-color: #e6f3ff; padding: 2px;'>Normal</span> <span style='background-color: #ffe6cc; padding: 2px;'>Med. increase</span> <span style='background-color: #ffcccc; padding: 2px;'>Large increase</span> <span style='background-color: #e6ffcc; padding: 2px;'>Med. decrease</span> <span style='background-color: #ccffcc; padding: 2px;'>Large decrease</span></small>\"\n",
    "                \n",
    "                # Update the graph with current predictions\n",
    "                if len(prediction_history) > 1:\n",
    "                    # Create hover text with context tokens\n",
    "                    hover_texts = []\n",
    "                    for i in range(len(generated_tokens)):\n",
    "                        # Get 5 tokens before and after (if available)\n",
    "                        start_idx = max(0, i - 5)\n",
    "                        end_idx = min(len(generated_tokens), i + 6)\n",
    "                        \n",
    "                        context_tokens = []\n",
    "                        for j in range(start_idx, end_idx):\n",
    "                            token_clean = generated_tokens[j].replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '\"').replace('&#x27;', \"'\")\n",
    "                            if j == i:\n",
    "                                context_tokens.append(f\"<b>{token_clean}</b>\")\n",
    "                            else:\n",
    "                                context_tokens.append(token_clean)\n",
    "                        \n",
    "                        context_str = \" \".join(context_tokens)\n",
    "                        hover_text = f\"Token {i+1}: {context_str}<br>Predicted Total: {prediction_history[i]:.0f}\"\n",
    "                        hover_texts.append(hover_text)\n",
    "                    \n",
    "                    # Update graph with new data\n",
    "                    graph_widget.data = []\n",
    "                    graph_widget.add_trace(go.Scatter(\n",
    "                        x=token_counts,\n",
    "                        y=prediction_history,\n",
    "                        mode='lines+markers',\n",
    "                        name='Predicted Total Tokens',\n",
    "                        line=dict(color='blue', width=2),\n",
    "                        marker=dict(size=6),\n",
    "                        hovertemplate='%{customdata}<extra></extra>',\n",
    "                        customdata=hover_texts\n",
    "                    ))\n",
    "                    \n",
    "                    # Add a horizontal line showing actual tokens generated so far\n",
    "                    graph_widget.add_trace(go.Scatter(\n",
    "                        x=[token_counts[0], token_counts[-1]],\n",
    "                        y=[n_tokens_generated, n_tokens_generated],\n",
    "                        mode='lines',\n",
    "                        name='Current Progress',\n",
    "                        line=dict(color='red', width=2, dash='dash'),\n",
    "                        hovertemplate='Current tokens generated: %{y}<extra></extra>'\n",
    "                    ))\n",
    "                    \n",
    "                    # Update layout\n",
    "                    graph_widget.update_layout(\n",
    "                        title='Token Prediction Over Time',\n",
    "                        xaxis_title='Token Number',\n",
    "                        yaxis_title='Predicted Total Tokens',\n",
    "                        height=400,\n",
    "                        showlegend=True,\n",
    "                        hovermode='closest'\n",
    "                    )\n",
    "    \n",
    "    # Store final token count globally\n",
    "    stored_n_tokens = n_tokens_generated\n",
    "    \n",
    "    # After generation is complete, display prediction history\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL PREDICTION HISTORY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nGenerated {n_tokens_generated} tokens total\")\n",
    "    print(f\"Final prediction was {predicted_total_tokens:.0f} tokens\")\n",
    "    print(f\"Accuracy: {(n_tokens_generated/predicted_total_tokens)*100:.1f}%\")\n",
    "    print(f\"EMA factor used: {ema_factor}\")\n",
    "    \n",
    "    print(\"\\nFull Prediction History:\")\n",
    "    print(\"Token# | Predicted Total | Change | % Change | Token\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for i, (count, pred_total, token) in enumerate(zip(token_counts, prediction_history, generated_tokens)):\n",
    "        if i == 0:\n",
    "            change = 0\n",
    "            percent_change = 0\n",
    "        else:\n",
    "            change = pred_total - prediction_history[i-1]\n",
    "            percent_change = abs(change / prediction_history[i-1]) * 100 if prediction_history[i-1] != 0 else 0\n",
    "        \n",
    "        # Display token in a safe way for console output (remove escaping for print)\n",
    "        token_for_print = token.replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '\"').replace('&#x27;', \"'\")\n",
    "        \n",
    "        # Highlight tokens with large percentage changes (>10%)\n",
    "        if percent_change > 10:\n",
    "            if change > 0:\n",
    "                # Large increase - bold green\n",
    "                token_display_str = f\"\\033[1m\\033[92m{token_for_print}\\033[0m\"\n",
    "                change_str = f\"\\033[1m\\033[92m+{change:.0f}\\033[0m\"\n",
    "            else:\n",
    "                # Large decrease - bold red  \n",
    "                token_display_str = f\"\\033[1m\\033[91m{token_for_print}\\033[0m\"\n",
    "                change_str = f\"\\033[1m\\033[91m{change:.0f}\\033[0m\"\n",
    "        else:\n",
    "            token_display_str = token_for_print\n",
    "            if change > 0:\n",
    "                change_str = f\"+{change:.0f}\"\n",
    "            else:\n",
    "                change_str = f\"{change:.0f}\"\n",
    "        \n",
    "        print(f\"{count:5d}  | {pred_total:13.0f}   | {change_str:8s} | {percent_change:6.1f}%  | {token_display_str}\")\n",
    "    \n",
    "    # Update graph with final results using current EMA\n",
    "    update_graph_with_ema(ema_factor)\n",
    "\n",
    "# Connect button click to function\n",
    "submit_button.on_click(on_submit_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd72caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install anywidget"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
