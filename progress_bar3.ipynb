{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f73bb0e",
   "metadata": {},
   "source": [
    "# Progress Bar with Linear Regression Probe\n",
    "\n",
    "This notebook uses a simple linear probe that directly predicts log(tokens_remaining) instead of classification with binning.\n",
    "\n",
    "Just run all these cells"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb495f6",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f50b3b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in ./.venv/lib/python3.11/site-packages (8.1.7)\n",
      "Requirement already satisfied: plotly in ./.venv/lib/python3.11/site-packages (6.3.1)\n",
      "Requirement already satisfied: anywidget in ./.venv/lib/python3.11/site-packages (0.9.18)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (9.6.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in ./.venv/lib/python3.11/site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in ./.venv/lib/python3.11/site-packages (from plotly) (2.6.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.11/site-packages (from plotly) (25.0)\n",
      "Requirement already satisfied: psygnal>=0.8.1 in ./.venv/lib/python3.11/site-packages (from anywidget) (0.14.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.11/site-packages (from anywidget) (4.15.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: psygnal>=0.8.1 in ./.venv/lib/python3.11/site-packages (from anywidget) (0.14.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./.venv/lib/python3.11/site-packages (from anywidget) (4.15.0)\n",
      "Requirement already satisfied: decorator in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in ./.venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./.venv/lib/python3.11/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in ./.venv/lib/python3.11/site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ipywidgets plotly anywidget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af922904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from einops import einsum\n",
    "import torch\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import time\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d0d918",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23020c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c6b5eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probe weights shape: torch.Size([1, 2560])\n"
     ]
    }
   ],
   "source": [
    "# Load linear regression probe weights (1D vector for direct prediction)\n",
    "probe_weights = torch.load('/root/llm-progress-monitor/models/probe_weights.pt')\n",
    "print(f\"Probe weights shape: {probe_weights.shape}\")\n",
    "model_name = 'Qwen/Qwen3-4B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59988a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(model_name, device_map=device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0fc4fc",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe0acd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ema_preds(log_preds, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Apply exponential moving average to predictions.\n",
    "    log_preds: tensor of log(tokens_remaining) predictions\n",
    "    \"\"\"\n",
    "    given_alpha = alpha\n",
    "    preds_list = log_preds.exp().tolist()\n",
    "    \n",
    "    ema_preds = []\n",
    "    cur_ema = None\n",
    "    for i, pred in enumerate(preds_list):\n",
    "        alpha = given_alpha\n",
    "        if cur_ema is None:\n",
    "            cur_ema = pred\n",
    "        else:\n",
    "            cur_ema = alpha*(cur_ema-1) + (1-alpha)*pred  # -1 because we have stepped one token\n",
    "        ema_preds.append(cur_ema)\n",
    "    return ema_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b889c645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_preds(activation, probe_weights):\n",
    "    \"\"\"\n",
    "    Get log(tokens_remaining) predictions using linear regression probe.\n",
    "    \n",
    "    Args:\n",
    "        activation: [seq, d_model] tensor of model activations\n",
    "        probe_weights: [n_probes, d_model] or [d_model] tensor of linear probe weights\n",
    "    \n",
    "    Returns:\n",
    "        [seq] tensor of log(tokens_remaining) predictions\n",
    "    \"\"\"\n",
    "    # Handle 2D probe weights (squeeze to 1D if it's [1, d_model])\n",
    "    if len(probe_weights.shape) == 2:\n",
    "        if probe_weights.shape[0] == 1:\n",
    "            # Squeeze to 1D: [1, d_model] -> [d_model]\n",
    "            probe_weights = probe_weights.squeeze(0)\n",
    "        else:\n",
    "            # If multiple probes, use the first one\n",
    "            probe_weights = probe_weights[0]\n",
    "    \n",
    "    # Simple dot product for direct regression prediction\n",
    "    log_preds = einsum(\n",
    "        activation, \n",
    "        probe_weights, \n",
    "        'seq d_model, d_model -> seq'\n",
    "    )\n",
    "    \n",
    "    return log_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ef0cb7",
   "metadata": {},
   "source": [
    "# Interactive UI with Progress Tracking and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeed28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a141961387746e4878aab97f4c32327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>Text Generation Progress (Linear Regression Probe)</h3>'), Textarea(value='', d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f38f80781741b39824dafe47568fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import ipywidgets as widgets\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Create input text box for prompt\n",
    "prompt_input = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    placeholder='Enter your prompt here...',\n",
    "    description='Prompt:',\n",
    "    layout=widgets.Layout(width='100%', height='80px')\n",
    ")\n",
    "\n",
    "# Create input text box for EMA factor\n",
    "ema_input = widgets.FloatText(\n",
    "    value=0.9,\n",
    "    description='EMA Factor:',\n",
    "    min=0.0,\n",
    "    max=1.0,\n",
    "    step=0.01,\n",
    "    tooltip='Exponential Moving Average factor (0.0 to 1.0)',\n",
    "    layout=widgets.Layout(width='200px')\n",
    ")\n",
    "\n",
    "# Create submit button\n",
    "submit_button = widgets.Button(\n",
    "    description='Generate Text',\n",
    "    button_style='success',\n",
    "    tooltip='Click to start text generation',\n",
    "    icon='play'\n",
    ")\n",
    "\n",
    "# Create progress bar widget\n",
    "progress_bar = widgets.FloatProgress(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,\n",
    "    description='Progress:',\n",
    "    bar_style='info',\n",
    "    style={'bar_color': '#20B2AA'},\n",
    "    orientation='horizontal'\n",
    ")\n",
    "\n",
    "# Create percentage label\n",
    "percentage_label = widgets.HTML(\n",
    "    value=\"<b>0.0%</b>\",\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create horizontal box for progress bar and percentage\n",
    "progress_row = widgets.HBox([progress_bar, percentage_label])\n",
    "\n",
    "# Create text widget for token display\n",
    "token_display = widgets.HTML(\n",
    "    value=\"<b>Generated tokens will appear here...</b>\",\n",
    "    placeholder='',\n",
    "    description='',\n",
    ")\n",
    "\n",
    "# Create graph widget for prediction history\n",
    "graph_widget = go.FigureWidget()\n",
    "graph_widget.update_layout(\n",
    "    title='Token Prediction Over Time',\n",
    "    xaxis_title='Token Number',\n",
    "    yaxis_title='Predicted Total Tokens',\n",
    "    height=400,\n",
    "    showlegend=True,\n",
    "    hovermode='closest',\n",
    "    margin=dict(l=50, r=50, t=50, b=50)\n",
    ")\n",
    "\n",
    "# Create container for the widgets\n",
    "progress_container = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Text Generation Progress (Linear Regression Probe)</h3>\"),\n",
    "    prompt_input,\n",
    "    ema_input,\n",
    "    submit_button,\n",
    "    progress_row,\n",
    "    token_display,\n",
    "    widgets.HTML(\"<h4>Prediction History</h4>\"),\n",
    "    graph_widget\n",
    "])\n",
    "\n",
    "# Display the widget\n",
    "display(progress_container)\n",
    "\n",
    "# Global variables to store generation data\n",
    "raw_log_preds = []\n",
    "stored_generated_tokens = []\n",
    "stored_n_tokens = 0\n",
    "\n",
    "def update_graph_with_ema(ema_factor):\n",
    "    \"\"\"Update the graph and displays with a new EMA factor.\"\"\"\n",
    "    global raw_log_preds, stored_generated_tokens, stored_n_tokens\n",
    "    \n",
    "    if len(raw_log_preds) == 0:\n",
    "        return\n",
    "    \n",
    "    # Recalculate predictions with new EMA\n",
    "    ema_preds = get_ema_preds(torch.tensor(raw_log_preds), alpha=ema_factor)\n",
    "    \n",
    "    prediction_history = []\n",
    "    token_counts = []\n",
    "    \n",
    "    for i, ema_pred in enumerate(ema_preds):\n",
    "        n_tokens_generated = i + 1\n",
    "        pred_tokens_remaining = ema_pred\n",
    "        predicted_total_tokens = n_tokens_generated + pred_tokens_remaining\n",
    "        prediction_history.append(predicted_total_tokens)\n",
    "        token_counts.append(n_tokens_generated)\n",
    "    \n",
    "    # Update the graph\n",
    "    if len(prediction_history) > 1:\n",
    "        # Create hover text with context tokens\n",
    "        hover_texts = []\n",
    "        for i in range(len(stored_generated_tokens)):\n",
    "            # Get 5 tokens before and after (if available)\n",
    "            start_idx = max(0, i - 5)\n",
    "            end_idx = min(len(stored_generated_tokens), i + 6)\n",
    "            \n",
    "            context_tokens = []\n",
    "            for j in range(start_idx, end_idx):\n",
    "                token_clean = stored_generated_tokens[j].replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '\"').replace('&#x27;', \"'\")\n",
    "                if j == i:\n",
    "                    context_tokens.append(f\"<b>{token_clean}</b>\")\n",
    "                else:\n",
    "                    context_tokens.append(token_clean)\n",
    "            \n",
    "            context_str = \" \".join(context_tokens)\n",
    "            hover_text = f\"Token {i+1}: {context_str}<br>Predicted Total: {prediction_history[i]:.0f}\"\n",
    "            hover_texts.append(hover_text)\n",
    "        \n",
    "        # Update graph with new data using batch_update for better performance\n",
    "        with graph_widget.batch_update():\n",
    "            graph_widget.data = []\n",
    "            graph_widget.add_trace(go.Scatter(\n",
    "                x=token_counts,\n",
    "                y=prediction_history,\n",
    "                mode='lines+markers',\n",
    "                name='Predicted Total Tokens',\n",
    "                line=dict(color='blue', width=2),\n",
    "                marker=dict(size=6),\n",
    "                hovertemplate='%{customdata}<extra></extra>',\n",
    "                customdata=hover_texts\n",
    "            ))\n",
    "            \n",
    "            # Add actual final point\n",
    "            graph_widget.add_trace(go.Scatter(\n",
    "                x=[stored_n_tokens],\n",
    "                y=[stored_n_tokens],\n",
    "                mode='markers',\n",
    "                name='Actual Final',\n",
    "                marker=dict(size=10, color='green', symbol='star'),\n",
    "                hovertemplate=f'Actual completion: {stored_n_tokens} tokens<extra></extra>'\n",
    "            ))\n",
    "            \n",
    "            # Update layout\n",
    "            graph_widget.update_layout(\n",
    "                title=f'Token Prediction vs Reality (EMA={ema_factor:.2f})',\n",
    "                xaxis_title='Token Number',\n",
    "                yaxis_title='Predicted Total Tokens',\n",
    "                height=400,\n",
    "                showlegend=True,\n",
    "                hovermode='closest'\n",
    "            )\n",
    "        \n",
    "        # Update token display with final statistics\n",
    "        final_pred = prediction_history[-1]\n",
    "        accuracy = (stored_n_tokens/final_pred)*100\n",
    "        token_display.value = f\"<b>Generation complete!</b><br><b>Total tokens:</b> {stored_n_tokens}<br><b>Final prediction:</b> {final_pred:.0f} tokens<br><b>Accuracy:</b> {accuracy:.1f}%<br><b>Current EMA:</b> {ema_factor:.2f}\"\n",
    "\n",
    "def on_ema_changed(change):\n",
    "    \"\"\"Handle EMA input changes.\"\"\"\n",
    "    update_graph_with_ema(change['new'])\n",
    "\n",
    "# Connect EMA input to update function\n",
    "ema_input.observe(on_ema_changed, names='value')\n",
    "\n",
    "def on_submit_clicked(b):\n",
    "    global raw_log_preds, stored_generated_tokens, stored_n_tokens\n",
    "    \n",
    "    # Reset progress\n",
    "    progress_bar.value = 0\n",
    "    percentage_label.value = \"<b>0.0%</b>\"\n",
    "    token_display.value = \"<b>Generating...</b>\"\n",
    "    \n",
    "    # Clear the graph\n",
    "    graph_widget.data = []\n",
    "    \n",
    "    # Reset global storage\n",
    "    raw_log_preds = []\n",
    "    stored_generated_tokens = []\n",
    "    stored_n_tokens = 0\n",
    "    \n",
    "    # Initialize lists to track predictions over time\n",
    "    prediction_history = []\n",
    "    token_counts = []\n",
    "    \n",
    "    # Get prompt from input\n",
    "    prompt = prompt_input.value\n",
    "    # Get EMA factor from input\n",
    "    ema_factor = ema_input.value\n",
    "    \n",
    "    # Apply chat template\n",
    "    prompt = model.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": prompt}], tokenize=False, add_generation_prompt=True)\n",
    "    cur_log_preds = []\n",
    "    n_tokens_generated = 0\n",
    "    generated_tokens = []\n",
    "    predicted_total_tokens = 0  # Initialize to avoid UnboundLocalError\n",
    "    pred_percent_through = 0\n",
    "\n",
    "    with model.generate(prompt, max_new_tokens=32768, do_sample=True) as tracer:\n",
    "        # Call .all() to apply intervention to each new token\n",
    "        with tracer.all():\n",
    "            activations = model.model.layers[15].output[0]\n",
    "            if len(activations.shape) == 1:\n",
    "                activations = activations.unsqueeze(0)\n",
    "            \n",
    "            # Save predictions within nnsight context\n",
    "            preds_saved = get_log_preds(activations, probe_weights).save()\n",
    "            token_saved = model.lm_head.output.argmax(dim=-1).save()\n",
    "            \n",
    "            # Process using .tolist() which works token-by-token in nnsight\n",
    "            preds = preds_saved.tolist()\n",
    "            \n",
    "            # Only process single-token predictions (skip multi-token which are less common)\n",
    "            if not isinstance(preds, list) or len(preds) == 1:\n",
    "                # Extract single value\n",
    "                pred_value = preds[0] if isinstance(preds, list) else preds\n",
    "                \n",
    "                cur_log_preds.append(pred_value)\n",
    "                raw_log_preds.append(pred_value)  # Store raw predictions globally\n",
    "                \n",
    "                ema_preds = get_ema_preds(torch.tensor(cur_log_preds), alpha=ema_factor)\n",
    "                n_tokens_generated += 1\n",
    "                pred_tokens_remaining = ema_preds[-1]\n",
    "                predicted_total_tokens = n_tokens_generated + pred_tokens_remaining\n",
    "                pred_percent_through = n_tokens_generated/(n_tokens_generated + pred_tokens_remaining)\n",
    "                \n",
    "                # Store prediction data for highlighting\n",
    "                prediction_history.append(predicted_total_tokens)\n",
    "                token_counts.append(n_tokens_generated)\n",
    "                \n",
    "                token = token_saved.tolist()\n",
    "                token_str = model.tokenizer.decode(token[0][0], skip_special_tokens=False)\n",
    "                # Escape HTML entities in token string for safe display\n",
    "                token_str_escaped = token_str.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;').replace('\"', '&quot;').replace(\"'\", '&#x27;')\n",
    "                # Don't print during generation to avoid breaking the UI\n",
    "                # print(token_str, end='', flush=True)\n",
    "                generated_tokens.append(token_str_escaped)\n",
    "                stored_generated_tokens.append(token_str_escaped)  # Store globally\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.value = pred_percent_through * 100\n",
    "                \n",
    "                # Update percentage label\n",
    "                percentage_label.value = f\"<b>{pred_percent_through*100:.1f}%</b>\"\n",
    "                \n",
    "                # Create highlighted token display\n",
    "                highlighted_tokens = []\n",
    "                for i, token in enumerate(generated_tokens):\n",
    "                    # Calculate percentage change if we have history\n",
    "                    highlight_color = \"#e6f3ff\"  # Default light blue\n",
    "                    \n",
    "                    if i > 0 and i < len(prediction_history):\n",
    "                        change = prediction_history[i] - prediction_history[i-1]\n",
    "                        percent_change = abs(change / prediction_history[i-1]) * 100 if prediction_history[i-1] != 0 else 0\n",
    "                        \n",
    "                        # Color code based on prediction change magnitude\n",
    "                        if percent_change > 15:\n",
    "                            if change > 0:\n",
    "                                highlight_color = \"#ffcccc\"  # Light red for large increases\n",
    "                            else:\n",
    "                                highlight_color = \"#ccffcc\"  # Light green for large decreases\n",
    "                        elif percent_change > 5:\n",
    "                            if change > 0:\n",
    "                                highlight_color = \"#ffe6cc\"  # Light orange for medium increases\n",
    "                            else:\n",
    "                                highlight_color = \"#e6ffcc\"  # Light yellow-green for medium decreases\n",
    "                    \n",
    "                    highlighted_tokens.append(f\"<span style='background-color: {highlight_color}; padding: 2px 4px; margin: 1px; border-radius: 3px;'>{token}</span>\")\n",
    "                \n",
    "                tokens_html = \" \".join(highlighted_tokens)\n",
    "                token_display.value = f\"<b>Generated tokens:</b><br>{tokens_html}<br><br><b>Latest:</b> '{token_str_escaped}' | <b>Predicted Total:</b> {predicted_total_tokens:.0f} tokens | <b>Progress:</b> {pred_percent_through*100:.1f}%<br><br><small><b>Color coding:</b> <span style='background-color: #e6f3ff; padding: 2px;'>Normal</span> <span style='background-color: #ffe6cc; padding: 2px;'>Med. increase</span> <span style='background-color: #ffcccc; padding: 2px;'>Large increase</span> <span style='background-color: #e6ffcc; padding: 2px;'>Med. decrease</span> <span style='background-color: #ccffcc; padding: 2px;'>Large decrease</span></small>\"\n",
    "                \n",
    "                # Update the graph with current predictions (every token for real-time feedback)\n",
    "                if len(prediction_history) > 1:\n",
    "                    # Create hover text with context tokens\n",
    "                    hover_texts = []\n",
    "                    for i in range(len(generated_tokens)):\n",
    "                        # Get 5 tokens before and after (if available)\n",
    "                        start_idx = max(0, i - 5)\n",
    "                        end_idx = min(len(generated_tokens), i + 6)\n",
    "                        \n",
    "                        context_tokens = []\n",
    "                        for j in range(start_idx, end_idx):\n",
    "                            token_clean = generated_tokens[j].replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>').replace('&quot;', '\"').replace('&#x27;', \"'\")\n",
    "                            if j == i:\n",
    "                                context_tokens.append(f\"<b>{token_clean}</b>\")\n",
    "                            else:\n",
    "                                context_tokens.append(token_clean)\n",
    "                        \n",
    "                        context_str = \" \".join(context_tokens)\n",
    "                        hover_text = f\"Token {i+1}: {context_str}<br>Predicted Total: {prediction_history[i]:.0f}\"\n",
    "                        hover_texts.append(hover_text)\n",
    "                    \n",
    "                    # Update graph with new data\n",
    "                    # Create x-axis values directly to avoid proxy issues\n",
    "                    x_values = list(range(1, len(prediction_history) + 1))\n",
    "                    with graph_widget.batch_update():\n",
    "                        graph_widget.data = []\n",
    "                        graph_widget.add_trace(go.Scatter(\n",
    "                            x=x_values,\n",
    "                            y=prediction_history,\n",
    "                            mode='lines+markers',\n",
    "                            name='Predicted Total Tokens',\n",
    "                            line=dict(color='blue', width=2),\n",
    "                            marker=dict(size=6),\n",
    "                            hovertemplate='%{customdata}<extra></extra>',\n",
    "                            customdata=hover_texts\n",
    "                        ))\n",
    "                        \n",
    "                        # Add a horizontal line showing actual tokens generated so far\n",
    "                        graph_widget.add_trace(go.Scatter(\n",
    "                            x=[1, n_tokens_generated],\n",
    "                            y=[n_tokens_generated, n_tokens_generated],\n",
    "                            mode='lines',\n",
    "                            name='Current Progress',\n",
    "                            line=dict(color='red', width=2, dash='dash'),\n",
    "                            hovertemplate='Current tokens generated: %{y}<extra></extra>'\n",
    "                        ))\n",
    "                        \n",
    "                        # Update layout\n",
    "                        graph_widget.update_layout(\n",
    "                            title=f'Token Prediction Over Time (Live - {n_tokens_generated} tokens)',\n",
    "                            xaxis_title='Token Number',\n",
    "                            yaxis_title='Predicted Total Tokens',\n",
    "                            height=400,\n",
    "                            showlegend=True,\n",
    "                            hovermode='closest'\n",
    "                        )\n",
    "    \n",
    "    # Store final token count globally\n",
    "    stored_n_tokens = n_tokens_generated\n",
    "    \n",
    "    # Print completion message\n",
    "    print(f\"\\n✓ Generation complete! {n_tokens_generated} tokens generated\")\n",
    "    if predicted_total_tokens > 0:\n",
    "        print(f\"Final prediction: {predicted_total_tokens:.0f} tokens (Accuracy: {(n_tokens_generated/predicted_total_tokens)*100:.1f}%)\")\n",
    "    \n",
    "    # Update graph with final results using current EMA\n",
    "    update_graph_with_ema(ema_factor)\n",
    "\n",
    "# Connect button click to function\n",
    "submit_button.on_click(on_submit_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a509ea82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
